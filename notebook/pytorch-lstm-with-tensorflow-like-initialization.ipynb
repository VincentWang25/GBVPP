{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a03b14a9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.011638,
     "end_time": "2021-10-12T02:00:26.378851",
     "exception": false,
     "start_time": "2021-10-12T02:00:26.367213",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PyTorch LSTM with TensorFlow-like initialization\n",
    "\n",
    "My purpose for this notebook is to reproduce,\n",
    "\n",
    "Dmitry Uarov: https://www.kaggle.com/dmitryuarov/ventilator-pressure-eda-lstm-0-189/notebook\n",
    "\n",
    "with PyTorch (public LB 0.189). This sounds easy, but not always. I first got a significantly worse score ~ 0.3 using the same features and the model. Since I have heard that the weight initializations are different between PyTorch and TensorFlow, I am trying to make them as similar as I can in this notebook.\n",
    "\n",
    "The weight initializations are as follows, according to the official documents\n",
    "([Keras](https://keras.io/api/layers/recurrent_layers/lstm/), \n",
    "[PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)):\n",
    "\n",
    "| Parameter | TensorFlow/Keras | PyTorch |\n",
    "| ---       | --- | --- |\n",
    "| weight_ih | xavier uniform | uniform √hidden_size |\n",
    "| weight_hh | orthogonal     | same as above                    |\n",
    "| bias      | 1 for forget gate, 0 other wise | same as above    |\n",
    "| linear    | xavier uniform | uniform √input_size |\n",
    "\n",
    "I wrote `_reinitialize()` in class `Model`, which is the main content of this notebook.\n",
    "\n",
    "For me, using Xavier uniform for the fully connected (linear) after the LSTM was most important (which \n",
    "looks least important to me, though). TensorFlow initialization scheme for LSTM helped, too.\n",
    "\n",
    "Remaining uncertainties:\n",
    "\n",
    "* One LSTM weight is actually 4 matrices packed in one tensor. Should I initialize 4 matrices separately?\n",
    "* Two biases bi and bh in Pytorch LSTM seem redundant. For the forget gate, I only set one of them to 1 because I saw somewhere that Keras have only one bias, but I am not sure.\n",
    "\n",
    "Change log\n",
    "* Version 3: Public score is computed from 5 models (5 folds) trained locally. It was from 2 folds trained in notebook in version 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9446f888",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:50:26.866480Z",
     "start_time": "2021-10-13T14:50:25.955849Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:00:26.404232Z",
     "iopub.status.busy": "2021-10-12T02:00:26.402776Z",
     "iopub.status.idle": "2021-10-12T02:00:31.769920Z",
     "shell.execute_reply": "2021-10-12T02:00:31.769361Z",
     "shell.execute_reply.started": "2021-10-12T01:57:49.186647Z"
    },
    "papermill": {
     "duration": 5.380848,
     "end_time": "2021-10-12T02:00:31.770084",
     "exception": false,
     "start_time": "2021-10-12T02:00:26.389236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "debug = False\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11844292",
   "metadata": {
    "papermill": {
     "duration": 0.011016,
     "end_time": "2021-10-12T02:00:31.792103",
     "exception": false,
     "start_time": "2021-10-12T02:00:31.781087",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Features and Dataset\n",
    "\n",
    "From: https://www.kaggle.com/dmitryuarov/ventilator-pressure-eda-lstm-0-189/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3002cb1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:50:27.766927Z",
     "start_time": "2021-10-13T14:50:27.756349Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:00:31.830536Z",
     "iopub.status.busy": "2021-10-12T02:00:31.828777Z",
     "iopub.status.idle": "2021-10-12T02:00:31.831143Z",
     "shell.execute_reply": "2021-10-12T02:00:31.831556Z",
     "shell.execute_reply.started": "2021-10-12T01:57:54.246795Z"
    },
    "papermill": {
     "duration": 0.029085,
     "end_time": "2021-10-12T02:00:31.831673",
     "exception": false,
     "start_time": "2021-10-12T02:00:31.802588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['area'] = df['time_step'] * df['u_in']\n",
    "    df['area'] = df.groupby('breath_id')['area'].cumsum()\n",
    "\n",
    "    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n",
    "\n",
    "    df['u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n",
    "    df['u_in_lag4'] = df['u_in'].shift(4).fillna(0)\n",
    "\n",
    "    df['R'] = df['R'].astype(str)\n",
    "    df['C'] = df['C'].astype(str)\n",
    "    df = pd.get_dummies(df)\n",
    "\n",
    "    g = df.groupby('breath_id')['u_in']\n",
    "    df['ewm_u_in_mean'] = g.ewm(halflife=10).mean()\\\n",
    "                           .reset_index(level=0, drop=True)\n",
    "    df['ewm_u_in_std'] = g.ewm(halflife=10).std()\\\n",
    "                          .reset_index(level=0, drop=True)\n",
    "    df['ewm_u_in_corr'] = g.ewm(halflife=10).corr()\\\n",
    "                           .reset_index(level=0, drop=True)\n",
    "\n",
    "    df['rolling_10_mean'] = g.rolling(window=10, min_periods=1).mean()\\\n",
    "                             .reset_index(level=0, drop=True)\n",
    "    df['rolling_10_max'] = g.rolling(window=10, min_periods=1).max()\\\n",
    "                            .reset_index(level=0, drop=True)\n",
    "    df['rolling_10_std'] = g.rolling(window=10, min_periods=1).std()\\\n",
    "                            .reset_index(level=0, drop=True)\n",
    "\n",
    "    df['expand_mean'] = g.expanding(2).mean()\\\n",
    "                         .reset_index(level=0, drop=True)\n",
    "    df['expand_max'] = g.expanding(2).max()\\\n",
    "                        .reset_index(level=0, drop=True)\n",
    "    df['expand_std'] = g.expanding(2).std()\\\n",
    "                        .reset_index(level=0, drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    df.drop(['id', 'breath_id'], axis=1, inplace=True)\n",
    "    if 'pressure' in df.columns:\n",
    "        df.drop('pressure', axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        if y is None:\n",
    "            y = np.zeros(len(X), dtype=np.float32)\n",
    "\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.y = y.astype(np.float32)\n",
    "        self.w = w.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i], self.w[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd32b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:50:41.640614Z",
     "start_time": "2021-10-13T14:50:41.637747Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:00:31.858500Z",
     "iopub.status.busy": "2021-10-12T02:00:31.857935Z",
     "iopub.status.idle": "2021-10-12T02:05:28.752117Z",
     "shell.execute_reply": "2021-10-12T02:05:28.752538Z",
     "shell.execute_reply.started": "2021-10-12T01:57:54.264917Z"
    },
    "papermill": {
     "duration": 296.910751,
     "end_time": "2021-10-12T02:05:28.752693",
     "exception": false,
     "start_time": "2021-10-12T02:00:31.841942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 100*1024 if debug else None\n",
    "\n",
    "di = \"/media/vincentwang/Backup/kaggle_data/ventilator-pressure-prediction/\"\n",
    "train = pd.read_csv(di + 'train.csv', nrows=n)\n",
    "test = pd.read_csv(di + 'test.csv', nrows=n)\n",
    "submit = pd.read_csv(di + 'sample_submission.csv', nrows=n)\n",
    "\n",
    "\n",
    "input_size = 22#X_all.shape[2]\n",
    "\n",
    "#print(len(X_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9edcb9",
   "metadata": {
    "papermill": {
     "duration": 0.010735,
     "end_time": "2021-10-12T02:05:28.774347",
     "exception": false,
     "start_time": "2021-10-12T02:05:28.763612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ebf52c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:50:45.324981Z",
     "start_time": "2021-10-13T14:50:45.315578Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:05:28.810195Z",
     "iopub.status.busy": "2021-10-12T02:05:28.808624Z",
     "iopub.status.idle": "2021-10-12T02:05:28.810822Z",
     "shell.execute_reply": "2021-10-12T02:05:28.811221Z",
     "shell.execute_reply.started": "2021-10-12T01:57:57.715535Z"
    },
    "papermill": {
     "duration": 0.026389,
     "end_time": "2021-10-12T02:05:28.811336",
     "exception": false,
     "start_time": "2021-10-12T02:05:28.784947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        hidden = [400, 300, 200, 100]\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size, hidden[0],\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.lstm2 = nn.LSTM(2 * hidden[0], hidden[1],\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(2 * hidden[1], hidden[2],\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.lstm4 = nn.LSTM(2 * hidden[2], hidden[3],\n",
    "                             batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(2 * hidden[3], 50)\n",
    "        self.selu = nn.SELU()\n",
    "        self.fc2 = nn.Linear(50, 1)\n",
    "        self._reinitialize()\n",
    "\n",
    "    def _reinitialize(self):\n",
    "        \"\"\"\n",
    "        Tensorflow/Keras-like initialization\n",
    "        \"\"\"\n",
    "        for name, p in self.named_parameters():\n",
    "            if 'lstm' in name:\n",
    "                if 'weight_ih' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'weight_hh' in name:\n",
    "                    nn.init.orthogonal_(p.data)\n",
    "                elif 'bias_ih' in name:\n",
    "                    p.data.fill_(0)\n",
    "                    # Set forget-gate bias to 1\n",
    "                    n = p.size(0)\n",
    "                    p.data[(n // 4):(n // 2)].fill_(1)\n",
    "                elif 'bias_hh' in name:\n",
    "                    p.data.fill_(0)\n",
    "            elif 'fc' in name:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(p.data)\n",
    "                elif 'bias' in name:\n",
    "                    p.data.fill_(0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.selu(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac11b794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:50:46.126039Z",
     "start_time": "2021-10-13T14:50:46.024314Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:05:28.837005Z",
     "iopub.status.busy": "2021-10-12T02:05:28.836385Z",
     "iopub.status.idle": "2021-10-12T02:05:29.073883Z",
     "shell.execute_reply": "2021-10-12T02:05:29.074307Z",
     "shell.execute_reply.started": "2021-10-12T01:57:57.730349Z"
    },
    "papermill": {
     "duration": 0.252069,
     "end_time": "2021-10-12T02:05:29.074455",
     "exception": false,
     "start_time": "2021-10-12T02:05:28.822386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm1.weight_ih_l0               (1600, 22)\n",
      "lstm1.weight_hh_l0               (1600, 400)\n",
      "lstm1.bias_ih_l0                 (1600,)\n",
      "lstm1.bias_hh_l0                 (1600,)\n",
      "lstm1.weight_ih_l0_reverse       (1600, 22)\n",
      "lstm1.weight_hh_l0_reverse       (1600, 400)\n",
      "lstm1.bias_ih_l0_reverse         (1600,)\n",
      "lstm1.bias_hh_l0_reverse         (1600,)\n",
      "lstm2.weight_ih_l0               (1200, 800)\n",
      "lstm2.weight_hh_l0               (1200, 300)\n",
      "lstm2.bias_ih_l0                 (1200,)\n",
      "lstm2.bias_hh_l0                 (1200,)\n",
      "lstm2.weight_ih_l0_reverse       (1200, 800)\n",
      "lstm2.weight_hh_l0_reverse       (1200, 300)\n",
      "lstm2.bias_ih_l0_reverse         (1200,)\n",
      "lstm2.bias_hh_l0_reverse         (1200,)\n",
      "lstm3.weight_ih_l0               (800, 600)\n",
      "lstm3.weight_hh_l0               (800, 200)\n",
      "lstm3.bias_ih_l0                 (800,)\n",
      "lstm3.bias_hh_l0                 (800,)\n",
      "lstm3.weight_ih_l0_reverse       (800, 600)\n",
      "lstm3.weight_hh_l0_reverse       (800, 200)\n",
      "lstm3.bias_ih_l0_reverse         (800,)\n",
      "lstm3.bias_hh_l0_reverse         (800,)\n",
      "lstm4.weight_ih_l0               (400, 400)\n",
      "lstm4.weight_hh_l0               (400, 100)\n",
      "lstm4.bias_ih_l0                 (400,)\n",
      "lstm4.bias_hh_l0                 (400,)\n",
      "lstm4.weight_ih_l0_reverse       (400, 400)\n",
      "lstm4.weight_hh_l0_reverse       (400, 100)\n",
      "lstm4.bias_ih_l0_reverse         (400,)\n",
      "lstm4.bias_hh_l0_reverse         (400,)\n",
      "fc1.weight                       (50, 200)\n",
      "fc1.bias                         (50,)\n",
      "fc2.weight                       (1, 50)\n",
      "fc2.bias                         (1,)\n"
     ]
    }
   ],
   "source": [
    "model = Model(input_size)\n",
    "for name, p in model.named_parameters():\n",
    "    print('%-32s %s' % (name, tuple(p.shape)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aff34",
   "metadata": {
    "papermill": {
     "duration": 0.011264,
     "end_time": "2021-10-12T02:05:29.097038",
     "exception": false,
     "start_time": "2021-10-12T02:05:29.085774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "587b13ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:50:48.072519Z",
     "start_time": "2021-10-13T14:50:48.068236Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:05:29.127116Z",
     "iopub.status.busy": "2021-10-12T02:05:29.126611Z",
     "iopub.status.idle": "2021-10-12T02:05:29.129655Z",
     "shell.execute_reply": "2021-10-12T02:05:29.130419Z",
     "shell.execute_reply.started": "2021-10-12T01:57:57.978884Z"
    },
    "papermill": {
     "duration": 0.02246,
     "end_time": "2021-10-12T02:05:29.130554",
     "exception": false,
     "start_time": "2021-10-12T02:05:29.108094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "def evaluate(model, loader_val):\n",
    "    tb = time.time()\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    loss_sum = 0\n",
    "    score_sum = 0\n",
    "    n_sum = 0\n",
    "    y_pred_all = []\n",
    "\n",
    "    for ibatch, (x, y, w) in enumerate(loader_val):\n",
    "        n = y.size(0)\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        w = w.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x).squeeze()\n",
    "\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        n_sum += n\n",
    "        loss_sum += n*loss.item()\n",
    "        \n",
    "        y_pred_all.append(y_pred.cpu().detach().numpy())\n",
    "\n",
    "    loss_val = loss_sum / n_sum\n",
    "\n",
    "    model.train(was_training)\n",
    "\n",
    "    d = {'loss': loss_val,\n",
    "         'time': time.time() - tb,\n",
    "         'y_pred': np.concatenate(y_pred_all, axis=0)}\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3aa2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-14T11:53:37.359687Z",
     "start_time": "2021-10-13T14:51:31.547988Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T02:05:29.168407Z",
     "iopub.status.busy": "2021-10-12T02:05:29.160144Z",
     "iopub.status.idle": "2021-10-12T07:01:43.872565Z",
     "shell.execute_reply": "2021-10-12T07:01:43.873288Z",
     "shell.execute_reply.started": "2021-10-12T01:57:57.989611Z"
    },
    "papermill": {
     "duration": 17774.731948,
     "end_time": "2021-10-12T07:01:43.873612",
     "exception": false,
     "start_time": "2021-10-12T02:05:29.141664",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "epoch loss_train loss_val lr time\n",
      "  1  1.905934  3.746136 5.000e-04   176.6    9.2\n",
      "  2  1.063023  3.650968 5.000e-04   303.4   18.6\n",
      "  3  0.932291  3.527099 5.000e-04   430.6   27.8\n",
      "  4  0.871136  3.281261 5.000e-04   557.8   37.1\n",
      "  5  0.827874  3.058842 5.000e-04   687.4   46.9\n",
      "  6  0.797389  3.062494 5.000e-04   815.3   56.3\n",
      "  7  0.776537  2.898428 5.000e-04   943.4   65.6\n",
      "  8  0.756011  2.784392 5.000e-04  1071.3   75.3\n",
      "  9  0.730173  2.718992 5.000e-04  1199.2   84.6\n",
      " 10  0.712961  2.622559 5.000e-04  1328.3   94.0\n",
      " 11  0.704738  2.590038 5.000e-04  1458.3  103.6\n",
      " 12  0.682354  2.553261 5.000e-04  1588.9  113.1\n",
      " 13  0.669544  2.563180 5.000e-04  1718.6  122.4\n",
      " 14  0.667309  2.463154 5.000e-04  1847.4  131.8\n",
      " 15  0.647801  2.395836 5.000e-04  1979.0  141.5\n",
      " 16  0.642287  2.407362 5.000e-04  2108.4  151.0\n",
      " 17  0.627489  2.360328 5.000e-04  2236.9  160.4\n",
      " 18  0.633536  2.385020 5.000e-04  2365.9  169.7\n",
      " 19  0.621331  2.330669 5.000e-04  2494.4  179.1\n",
      " 20  0.602168  2.298681 5.000e-04  2623.2  188.5\n",
      " 21  0.602760  2.263528 5.000e-04  2750.1  197.7\n",
      " 22  0.584305  2.272274 5.000e-04  2877.0  207.0\n",
      " 23  0.586790  2.253175 5.000e-04  3003.6  216.2\n",
      " 24  0.581107  2.183317 5.000e-04  3130.2  225.5\n",
      " 25  0.577420  2.229728 5.000e-04  3256.8  234.7\n",
      " 26  0.570217  2.246716 5.000e-04  3383.4  244.0\n",
      " 27  0.567261  2.166728 5.000e-04  3510.2  253.2\n",
      " 28  0.554213  2.188929 5.000e-04  3636.8  262.5\n",
      " 29  0.547733  2.245866 5.000e-04  3763.5  271.7\n",
      " 30  0.537750  2.071172 5.000e-04  3890.0  281.0\n",
      " 31  0.527320  2.147808 5.000e-04  4016.6  290.3\n",
      " 32  0.522255  2.105596 5.000e-04  4143.4  299.5\n",
      " 33  0.514586  2.105691 5.000e-04  4270.0  308.8\n",
      " 34  0.509891  2.104977 5.000e-04  4396.7  318.0\n",
      " 35  0.538127  2.071345 5.000e-04  4523.2  327.3\n",
      " 36  0.502217  2.030677 5.000e-04  4650.1  336.5\n",
      " 37  0.494923  2.056699 5.000e-04  4777.0  345.8\n",
      " 38  0.498030  2.035514 5.000e-04  4904.0  355.1\n",
      " 39  0.490476  2.086754 5.000e-04  5031.1  364.4\n",
      " 40  0.509027  2.082763 5.000e-04  5158.2  373.7\n",
      " 41  0.499201  2.156373 5.000e-04  5285.3  382.9\n",
      " 42  0.489547  2.051923 5.000e-04  5412.6  392.2\n",
      " 43  0.494449  2.100274 5.000e-04  5539.6  401.5\n",
      " 44  0.482392  2.059766 5.000e-04  5666.6  410.8\n",
      " 45  0.471077  2.076988 5.000e-04  5793.3  420.0\n",
      " 46  0.454005  2.050104 5.000e-04  5920.1  429.3\n",
      " 47  0.486802  2.060921 5.000e-04  6047.0  438.6\n",
      " 48  0.429634  2.114794 2.500e-04  6173.7  447.8\n",
      " 49  0.415933  1.993954 2.500e-04  6300.4  457.1\n",
      " 50  0.412076  2.033498 2.500e-04  6426.9  466.4\n",
      " 51  0.392383  1.998980 2.500e-04  6553.7  475.6\n",
      " 52  0.396617  2.016444 2.500e-04  6680.7  484.9\n",
      " 53  0.400812  1.971423 2.500e-04  6811.9  495.1\n",
      " 54  0.391457  1.975363 2.500e-04  6949.3  505.0\n",
      " 55  0.384454  1.975526 2.500e-04  7087.5  515.0\n",
      " 56  0.399453  1.966953 2.500e-04  7225.8  525.1\n",
      " 57  0.385425  1.985003 2.500e-04  7357.8  534.3\n",
      " 58  0.391809  1.970694 2.500e-04  7491.0  544.5\n",
      " 59  0.397516  1.933957 2.500e-04  7633.1  554.5\n",
      " 60  0.378648  1.968284 2.500e-04  7775.7  566.4\n",
      " 61  0.379190  1.969813 2.500e-04  7918.6  576.4\n",
      " 62  0.386292  1.986674 2.500e-04  8062.5  586.7\n",
      " 63  0.367307  2.006940 2.500e-04  8201.4  596.7\n",
      " 64  0.369848  1.965354 2.500e-04  8339.0  606.7\n",
      " 65  0.359332  1.984983 2.500e-04  8476.6  616.8\n",
      " 66  0.371123  1.995009 2.500e-04  8615.5  626.1\n",
      " 67  0.353249  1.972730 2.500e-04  8742.6  635.4\n",
      " 68  0.351887  2.006771 2.500e-04  8876.9  645.1\n",
      " 69  0.360506  1.974793 2.500e-04  9012.0  654.6\n",
      " 70  0.353946  1.949604 2.500e-04  9149.8  664.5\n",
      " 71  0.330862  1.961040 1.250e-04  9284.8  674.1\n",
      " 72  0.321391  1.976060 1.250e-04  9419.2  684.1\n",
      " 73  0.319843  1.957852 1.250e-04  9552.4  693.7\n",
      " 74  0.318599  1.998642 1.250e-04  9687.0  703.1\n",
      " 75  0.321899  1.988299 1.250e-04  9819.1  713.1\n",
      " 76  0.312948  1.968242 1.250e-04  9958.8  723.0\n",
      " 77  0.308971  1.971256 1.250e-04 10096.9  733.3\n",
      " 78  0.308780  1.982255 1.250e-04 10234.5  743.8\n",
      " 79  0.312866  1.973549 1.250e-04 10373.9  753.4\n",
      " 80  0.312206  1.992876 1.250e-04 10513.4  763.1\n",
      " 81  0.307225  1.970919 1.250e-04 10646.1  772.8\n",
      " 82  0.293533  1.972168 6.250e-05 10783.2  782.7\n",
      " 83  0.287613  1.968335 6.250e-05 10919.0  792.7\n",
      " 84  0.286249  1.964912 6.250e-05 11049.8  802.0\n",
      " 85  0.285623  1.968998 6.250e-05 11182.6  811.6\n",
      " 86  0.284021  1.968713 6.250e-05 11327.9  823.3\n",
      " 87  0.285559  1.966282 6.250e-05 11463.9  832.7\n",
      " 88  0.281736  1.972527 6.250e-05 11594.4  842.0\n",
      " 89  0.279425  1.967152 6.250e-05 11735.3  852.9\n",
      " 90  0.279486  1.971829 6.250e-05 11876.5  864.3\n",
      " 91  0.278763  1.960848 6.250e-05 12029.6  875.3\n",
      " 92  0.282269  1.966073 6.250e-05 12186.2  886.3\n",
      " 93  0.274771  1.962373 3.125e-05 12325.8  896.4\n",
      " 94  0.272049  1.966561 3.125e-05 12464.9  906.5\n",
      " 95  0.270418  1.961968 3.125e-05 12604.3  916.7\n",
      " 96  0.270964  1.964353 3.125e-05 12743.8  926.8\n",
      " 97  0.268992  1.966566 3.125e-05 12884.1  936.9\n",
      " 98  0.267956  1.963674 3.125e-05 13023.1  947.0\n",
      " 99  0.269921  1.961765 3.125e-05 13163.6  957.1\n",
      "100  0.267777  1.962711 3.125e-05 13303.4  967.2\n",
      "101  0.266359  1.963798 3.125e-05 13446.4  978.3\n",
      "102  0.266174  1.962236 3.125e-05 13587.5  988.4\n",
      "103  0.267618  1.966345 3.125e-05 13726.7  998.5\n",
      "104  0.263368  1.968871 1.563e-05 13865.7 1008.6\n",
      "105  0.262982  1.960702 1.563e-05 14004.9 1018.7\n",
      "106  0.261817  1.957151 1.563e-05 14144.1 1028.8\n",
      "107  0.261430  1.957602 1.563e-05 14287.4 1039.4\n",
      "108  0.261144  1.958652 1.563e-05 14424.9 1049.4\n",
      "109  0.260717  1.959032 1.563e-05 14568.8 1060.9\n",
      "110  0.260440  1.960553 1.563e-05 14715.4 1070.9\n",
      "111  0.260159  1.961117 1.563e-05 14853.7 1080.9\n",
      "112  0.259734  1.961978 1.563e-05 14990.8 1090.9\n",
      "113  0.259351  1.961671 1.563e-05 15127.2 1100.2\n",
      "114  0.258980  1.961208 1.563e-05 15254.4 1109.5\n",
      "115  0.257944  1.959090 7.813e-06 15381.4 1118.8\n",
      "116  0.257570  1.960897 7.813e-06 15508.4 1128.0\n",
      "117  0.257340  1.961828 7.813e-06 15635.3 1137.3\n",
      "118  0.257182  1.960858 7.813e-06 15762.1 1146.6\n",
      "119  0.256956  1.959365 7.813e-06 15888.8 1155.9\n",
      "120  0.256723  1.959532 7.813e-06 16015.6 1165.1\n",
      "121  0.256522  1.961398 7.813e-06 16142.4 1174.4\n",
      "122  0.256344  1.960508 7.813e-06 16269.1 1183.6\n",
      "123  0.256123  1.958965 7.813e-06 16396.1 1192.9\n",
      "124  0.255960  1.957724 7.813e-06 16523.1 1202.2\n",
      "125  0.255790  1.959133 7.813e-06 16650.2 1211.5\n",
      "126  0.255092  1.959972 3.906e-06 16777.2 1220.8\n",
      "127  0.254961  1.959435 3.906e-06 16904.3 1230.1\n",
      "128  0.254906  1.959546 3.906e-06 17031.3 1239.3\n",
      "129  0.254799  1.959238 3.906e-06 17158.2 1248.6\n",
      "130  0.254759  1.960854 3.906e-06 17285.1 1257.9\n",
      "131  0.254620  1.960844 3.906e-06 17412.2 1267.2\n",
      "132  0.254530  1.961166 3.906e-06 17539.3 1276.4\n",
      "133  0.254355  1.960479 3.906e-06 17666.4 1285.7\n",
      "134  0.254268  1.960156 3.906e-06 17793.5 1295.0\n",
      "135  0.254184  1.959456 3.906e-06 17920.4 1304.3\n",
      "136  0.254133  1.960176 3.906e-06 18047.2 1313.6\n",
      "137  0.253796  1.960595 1.953e-06 18174.0 1322.9\n",
      "138  0.253742  1.960810 1.953e-06 18300.8 1332.1\n",
      "139  0.253626  1.960146 1.953e-06 18427.6 1341.4\n",
      "140  0.253571  1.959835 1.953e-06 18554.4 1350.6\n",
      "141  0.253556  1.961090 1.953e-06 18681.2 1359.9\n",
      "142  0.253471  1.960437 1.953e-06 18808.0 1369.2\n",
      "143  0.253436  1.960008 1.953e-06 18934.9 1378.4\n",
      "144  0.253370  1.959465 1.953e-06 19061.7 1387.7\n",
      "145  0.253326  1.959693 1.953e-06 19188.5 1397.0\n",
      "146  0.253279  1.960127 1.953e-06 19315.3 1406.2\n",
      "147  0.253235  1.959800 1.953e-06 19442.1 1415.5\n",
      "148  0.253095  1.959970 9.766e-07 19568.9 1424.8\n",
      "149  0.253011  1.960060 9.766e-07 19695.9 1434.0\n",
      "150  0.252965  1.960402 9.766e-07 19823.0 1443.3\n",
      "151  0.252996  1.959429 9.766e-07 19950.0 1452.6\n",
      "152  0.252880  1.959614 9.766e-07 20077.0 1461.9\n",
      "153  0.252911  1.959483 9.766e-07 20204.1 1471.2\n",
      "154  0.252894  1.960070 9.766e-07 20331.1 1480.5\n",
      "155  0.252858  1.960082 9.766e-07 20458.2 1489.8\n",
      "156  0.252846  1.959992 9.766e-07 20585.4 1499.1\n",
      "157  0.252818  1.959561 9.766e-07 20712.3 1508.3\n",
      "158  0.252780  1.959851 9.766e-07 20839.2 1517.6\n",
      "159  0.252757  1.959451 4.883e-07 20966.0 1526.9\n",
      "160  0.252697  1.959626 4.883e-07 21093.1 1536.1\n",
      "161  0.252649  1.959655 4.883e-07 21220.3 1545.4\n",
      "162  0.252700  1.959772 4.883e-07 21347.5 1554.7\n",
      "163  0.252606  1.959507 4.883e-07 21474.5 1564.0\n",
      "164  0.252669  1.959936 4.883e-07 21601.3 1573.3\n",
      "165  0.252583  1.959602 4.883e-07 21728.3 1582.5\n",
      "166  0.252638  1.959699 4.883e-07 21855.1 1591.8\n",
      "167  0.252565  1.959605 4.883e-07 21981.9 1601.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168  0.252533  1.959643 4.883e-07 22108.7 1610.3\n",
      "169  0.252583  1.959097 4.883e-07 22235.5 1619.6\n",
      "170  0.252569  1.959496 2.441e-07 22362.3 1628.9\n",
      "171  0.252497  1.959581 2.441e-07 22489.1 1638.1\n",
      "172  0.252486  1.959463 2.441e-07 22615.7 1647.4\n",
      "173  0.252402  1.959539 2.441e-07 22742.6 1656.7\n",
      "174  0.252548  1.959600 2.441e-07 22869.3 1665.9\n",
      "175  0.252542  1.959644 2.441e-07 22996.1 1675.2\n",
      "176  0.252369  1.959622 2.441e-07 23122.9 1684.4\n",
      "177  0.252441  1.959573 2.441e-07 23249.6 1693.7\n",
      "178  0.252503  1.959588 2.441e-07 23376.3 1703.0\n",
      "179  0.252444  1.959537 2.441e-07 23503.2 1712.2\n",
      "180  0.252485  1.959716 2.441e-07 23630.1 1721.5\n",
      "181  0.252436  1.959560 1.221e-07 23757.0 1730.8\n",
      "182  0.252405  1.959536 1.221e-07 23883.7 1740.0\n",
      "183  0.252400  1.959683 1.221e-07 24010.5 1749.3\n",
      "184  0.252435  1.959549 1.221e-07 24137.1 1758.5\n",
      "185  0.252404  1.959650 1.221e-07 24263.9 1767.8\n",
      "186  0.252417  1.959495 1.221e-07 24390.7 1777.0\n",
      "187  0.252412  1.959487 1.221e-07 24517.3 1786.3\n",
      "188  0.252437  1.959575 1.221e-07 24644.0 1795.6\n",
      "189  0.252376  1.959665 1.221e-07 24770.7 1804.9\n",
      "190  0.252376  1.959555 1.221e-07 24897.6 1814.1\n",
      "191  0.252376  1.959560 1.221e-07 25024.6 1823.4\n",
      "192  0.252366  1.959536 6.104e-08 25151.7 1832.7\n",
      "193  0.252366  1.959569 6.104e-08 25278.9 1842.0\n",
      "194  0.252403  1.959563 6.104e-08 25405.9 1851.3\n",
      "195  0.252347  1.959589 6.104e-08 25532.7 1860.5\n",
      "196  0.252317  1.959539 6.104e-08 25659.5 1869.8\n",
      "197  0.252360  1.959503 6.104e-08 25787.4 1879.2\n",
      "198  0.252300  1.959522 6.104e-08 25916.6 1888.9\n",
      "199  0.252339  1.959521 6.104e-08 26052.9 1898.2\n",
      "200  0.252395  1.959486 6.104e-08 26180.0 1907.5\n",
      "201  0.252313  1.959553 6.104e-08 26306.9 1916.7\n",
      "202  0.252340  1.959578 6.104e-08 26434.1 1926.0\n",
      "203  0.252380  1.959511 3.052e-08 26561.1 1935.4\n",
      "204  0.252364  1.959523 3.052e-08 26688.0 1944.6\n",
      "205  0.252334  1.959514 3.052e-08 26815.1 1953.9\n",
      "206  0.252285  1.959530 3.052e-08 26942.0 1963.2\n",
      "207  0.252389  1.959502 3.052e-08 27068.8 1972.5\n",
      "208  0.252378  1.959523 3.052e-08 27195.8 1981.8\n",
      "209  0.252338  1.959522 3.052e-08 27322.7 1991.0\n",
      "210  0.252333  1.959515 3.052e-08 27449.5 2000.3\n",
      "211  0.252369  1.959528 3.052e-08 27576.4 2009.6\n",
      "212  0.252355  1.959493 3.052e-08 27703.4 2018.9\n",
      "213  0.252355  1.959526 3.052e-08 27830.5 2028.1\n",
      "214  0.252350  1.959507 1.526e-08 27957.4 2037.4\n",
      "215  0.252325  1.959522 1.526e-08 28084.3 2046.7\n",
      "216  0.252358  1.959514 1.526e-08 28211.3 2056.0\n",
      "217  0.252336  1.959515 1.526e-08 28338.3 2065.2\n",
      "218  0.252333  1.959512 1.526e-08 28465.5 2074.5\n",
      "219  0.252366  1.959513 1.526e-08 28592.7 2083.8\n",
      "220  0.252330  1.959518 1.526e-08 28720.0 2093.2\n",
      "221  0.252377  1.959507 1.526e-08 28848.0 2102.5\n",
      "222  0.252373  1.959521 1.526e-08 28975.4 2111.8\n",
      "223  0.252319  1.959497 1.526e-08 29109.0 2121.2\n",
      "224  0.252296  1.959496 1.526e-08 29236.6 2130.5\n",
      "225  0.252352  1.959502 1.526e-08 29363.7 2139.8\n",
      "226  0.252313  1.959509 1.526e-08 29491.0 2149.1\n",
      "227  0.252367  1.959519 1.526e-08 29618.2 2158.4\n",
      "228  0.252412  1.959503 1.526e-08 29745.4 2167.7\n",
      "229  0.252321  1.959498 1.526e-08 29873.2 2177.5\n",
      "230  0.252267  1.959496 1.526e-08 30000.4 2186.8\n",
      "231  0.252379  1.959493 1.526e-08 30127.6 2196.1\n",
      "232  0.252364  1.959495 1.526e-08 30254.5 2205.4\n",
      "233  0.252357  1.959515 1.526e-08 30381.6 2214.7\n",
      "234  0.252340  1.959509 1.526e-08 30508.8 2224.0\n",
      "235  0.252370  1.959504 1.526e-08 30636.0 2233.3\n",
      "236  0.252332  1.959490 1.526e-08 30763.4 2242.6\n",
      "237  0.252307  1.959508 1.526e-08 30890.7 2251.9\n",
      "238  0.252335  1.959499 1.526e-08 31017.7 2261.2\n",
      "239  0.252381  1.959491 1.526e-08 31144.9 2270.5\n",
      "240  0.252319  1.959512 1.526e-08 31272.8 2279.8\n",
      "241  0.252385  1.959502 1.526e-08 31400.8 2289.1\n",
      "242  0.252349  1.959499 1.526e-08 31543.4 2299.7\n",
      "243  0.252391  1.959495 1.526e-08 31672.0 2309.0\n",
      "244  0.252349  1.959507 1.526e-08 31799.1 2318.3\n",
      "245  0.252308  1.959501 1.526e-08 31930.5 2327.5\n",
      "246  0.252316  1.959485 1.526e-08 32057.7 2336.8\n",
      "247  0.252335  1.959505 1.526e-08 32184.9 2346.1\n",
      "248  0.252292  1.959496 1.526e-08 32312.0 2355.4\n",
      "249  0.252356  1.959495 1.526e-08 32447.9 2365.6\n",
      "250  0.252384  1.959485 1.526e-08 32577.5 2374.9\n",
      "251  0.252338  1.959494 1.526e-08 32704.9 2384.2\n",
      "252  0.252316  1.959490 1.526e-08 32833.8 2393.5\n",
      "253  0.252351  1.959498 1.526e-08 32961.8 2402.8\n",
      "254  0.252389  1.959493 1.526e-08 33096.7 2412.1\n",
      "255  0.252314  1.959489 1.526e-08 33224.6 2422.0\n",
      "256  0.252365  1.959492 1.526e-08 33352.7 2431.4\n",
      "257  0.252373  1.959491 1.526e-08 33481.0 2440.9\n",
      "258  0.252368  1.959500 1.526e-08 33611.8 2450.5\n",
      "259  0.252360  1.959485 1.526e-08 33742.3 2459.8\n",
      "260  0.252301  1.959482 1.526e-08 33869.6 2469.1\n",
      "261  0.252357  1.959483 1.526e-08 33997.0 2478.4\n",
      "262  0.252291  1.959493 1.526e-08 34124.1 2487.7\n",
      "263  0.252303  1.959484 1.526e-08 34251.3 2497.0\n",
      "264  0.252384  1.959485 1.526e-08 34378.7 2506.3\n",
      "265  0.252350  1.959490 1.526e-08 34505.7 2515.6\n",
      "266  0.252343  1.959496 1.526e-08 34633.0 2524.9\n",
      "267  0.252310  1.959474 1.526e-08 34760.2 2534.2\n",
      "268  0.252344  1.959486 1.526e-08 34887.4 2543.5\n",
      "269  0.252335  1.959485 1.526e-08 35014.7 2552.8\n",
      "270  0.252354  1.959485 1.526e-08 35141.6 2562.1\n",
      "271  0.252349  1.959481 1.526e-08 35269.0 2571.4\n",
      "272  0.252278  1.959478 1.526e-08 35396.1 2580.7\n",
      "273  0.252319  1.959487 1.526e-08 35523.3 2590.0\n",
      "274  0.252284  1.959489 1.526e-08 35650.5 2599.2\n",
      "275  0.252307  1.959485 1.526e-08 35777.4 2608.5\n",
      "276  0.252305  1.959481 1.526e-08 35904.8 2617.8\n",
      "277  0.252313  1.959491 1.526e-08 36031.9 2627.1\n",
      "278  0.252339  1.959484 1.526e-08 36159.2 2636.4\n",
      "279  0.252332  1.959485 1.526e-08 36286.8 2645.8\n",
      "280  0.252273  1.959495 1.526e-08 36414.0 2655.1\n",
      "281  0.252327  1.959488 1.526e-08 36541.6 2664.4\n",
      "282  0.252304  1.959487 1.526e-08 36668.9 2673.7\n",
      "283  0.252341  1.959490 1.526e-08 36797.0 2683.7\n",
      "284  0.252306  1.959479 1.526e-08 36932.0 2693.3\n",
      "285  0.252314  1.959483 1.526e-08 37063.4 2702.6\n",
      "286  0.252357  1.959482 1.526e-08 37190.9 2711.9\n",
      "287  0.252310  1.959488 1.526e-08 37318.1 2721.2\n",
      "288  0.252262  1.959482 1.526e-08 37448.7 2730.5\n",
      "289  0.252345  1.959478 1.526e-08 37581.7 2740.2\n",
      "290  0.252309  1.959493 1.526e-08 37719.0 2750.0\n",
      "291  0.252295  1.959465 1.526e-08 37852.2 2759.5\n",
      "292  0.252276  1.959493 1.526e-08 37985.6 2769.4\n",
      "293  0.252343  1.959473 1.526e-08 38121.5 2779.4\n",
      "294  0.252306  1.959470 1.526e-08 38254.7 2789.1\n",
      "295  0.252309  1.959491 1.526e-08 38389.4 2798.7\n",
      "296  0.252293  1.959479 1.526e-08 38523.8 2808.3\n",
      "297  0.252319  1.959481 1.526e-08 38657.6 2818.0\n",
      "298  0.252278  1.959493 1.526e-08 38791.9 2827.7\n",
      "299  0.252324  1.959471 1.526e-08 38927.6 2837.3\n",
      "300  0.252289  1.959479 1.526e-08 39062.1 2848.0\n",
      "model0.pth written\n",
      "Fold 1\n",
      "epoch loss_train loss_val lr time\n",
      "  1  1.838465  3.544196 5.000e-04   185.8    9.5\n",
      "  2  1.062310  3.448555 5.000e-04   318.1   19.6\n",
      "  3  0.928721  3.390127 5.000e-04   450.7   29.2\n",
      "  4  0.873101  3.189765 5.000e-04   590.8   39.0\n",
      "  5  0.829572  3.122297 5.000e-04   724.5   48.7\n",
      "  6  0.799300  2.974439 5.000e-04   856.0   58.0\n",
      "  7  0.770417  2.840898 5.000e-04   990.3   67.5\n",
      "  8  0.755665  2.789704 5.000e-04  1124.5   77.3\n",
      "  9  0.734232  2.715566 5.000e-04  1257.4   87.8\n",
      " 10  0.710939  2.655430 5.000e-04  1390.3   97.8\n",
      " 11  0.698310  2.756920 5.000e-04  1521.0  107.7\n",
      " 12  0.686099  2.668723 5.000e-04  1651.9  117.2\n",
      " 13  0.679220  2.637816 5.000e-04  1783.8  126.6\n",
      " 14  0.665863  2.564433 5.000e-04  1914.9  136.0\n",
      " 15  0.660717  2.450157 5.000e-04  2047.7  146.0\n",
      " 16  0.633981  2.660585 5.000e-04  2176.6  155.3\n",
      " 17  0.629551  2.446583 5.000e-04  2305.4  164.7\n",
      " 18  0.633307  2.483577 5.000e-04  2440.6  174.2\n",
      " 19  0.607077  2.471080 5.000e-04  2574.7  183.5\n",
      " 20  0.598638  2.501927 5.000e-04  2709.4  193.1\n",
      " 21  0.591194  2.431216 5.000e-04  2838.0  202.5\n",
      " 22  0.601946  2.384982 5.000e-04  2965.9  211.8\n",
      " 23  0.602281  2.317096 5.000e-04  3094.3  221.2\n",
      " 24  0.566339  2.319214 5.000e-04  3222.9  230.6\n",
      " 25  0.564604  2.332809 5.000e-04  3350.9  239.9\n",
      " 26  0.563436  2.391607 5.000e-04  3479.0  249.3\n",
      " 27  0.557237  2.298924 5.000e-04  3606.7  258.7\n",
      " 28  0.566884  2.251339 5.000e-04  3734.9  268.1\n",
      " 29  0.536968  2.327119 5.000e-04  3862.8  277.4\n",
      " 30  0.541078  2.339067 5.000e-04  3990.2  286.7\n",
      " 31  0.523654  2.204332 5.000e-04  4117.9  296.1\n",
      " 32  0.516577  2.213551 5.000e-04  4245.5  305.4\n",
      " 33  0.523638  2.117565 5.000e-04  4373.1  314.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 34  0.503807  2.142084 5.000e-04  4500.9  324.1\n",
      " 35  0.507466  2.216067 5.000e-04  4628.5  333.4\n",
      " 36  0.485135  2.227127 5.000e-04  4756.3  342.7\n",
      " 37  0.487655  2.194291 5.000e-04  4883.9  352.1\n",
      " 38  0.486788  2.072204 5.000e-04  5011.4  361.4\n",
      " 39  0.486372  2.187230 5.000e-04  5139.0  370.7\n",
      " 40  0.464501  2.108014 5.000e-04  5266.3  380.0\n",
      " 41  0.488041  2.027762 5.000e-04  5393.7  389.3\n",
      " 42  0.467884  2.090287 5.000e-04  5521.0  398.7\n",
      " 43  0.454982  2.056932 5.000e-04  5648.5  408.0\n",
      " 44  0.445766  2.075593 5.000e-04  5776.0  417.3\n",
      " 45  0.442070  2.033876 5.000e-04  5903.4  426.6\n",
      " 46  0.450679  2.010076 5.000e-04  6030.9  435.9\n",
      " 47  0.450078  2.123770 5.000e-04  6158.2  445.2\n",
      " 48  0.428531  2.098929 5.000e-04  6285.7  454.6\n",
      " 49  0.433079  2.008862 5.000e-04  6413.3  463.9\n",
      " 50  0.439071  1.942460 5.000e-04  6540.6  473.2\n",
      " 51  0.443909  2.433024 5.000e-04  6668.0  482.5\n",
      " 52  0.490677  1.967506 5.000e-04  6795.3  491.8\n",
      " 53  0.449486  1.966113 5.000e-04  6922.8  501.2\n",
      " 54  0.429941  1.942678 5.000e-04  7050.3  510.5\n",
      " 55  0.412776  1.970402 5.000e-04  7177.6  519.8\n",
      " 56  0.415366  1.971977 5.000e-04  7305.0  529.1\n",
      " 57  0.427947  2.031535 5.000e-04  7432.2  538.4\n",
      " 58  0.397945  1.950305 5.000e-04  7559.5  547.7\n",
      " 59  0.398541  1.989120 5.000e-04  7687.0  557.0\n",
      " 60  0.394500  2.075657 5.000e-04  7814.1  566.3\n",
      " 61  0.415173  1.952539 5.000e-04  7941.4  575.6\n",
      " 62  0.359294  1.965324 2.500e-04  8068.5  585.0\n",
      " 63  0.358747  1.921123 2.500e-04  8195.9  594.3\n",
      " 64  0.356107  1.940898 2.500e-04  8323.4  603.6\n",
      " 65  0.348236  1.954439 2.500e-04  8450.7  612.9\n",
      " 66  0.331783  1.936694 2.500e-04  8578.3  622.2\n",
      " 67  0.335769  1.967028 2.500e-04  8705.6  631.5\n",
      " 68  0.338431  1.980462 2.500e-04  8833.1  640.8\n",
      " 69  0.334028  1.925308 2.500e-04  8960.6  650.2\n",
      " 70  0.351491  1.947455 2.500e-04  9088.0  659.5\n",
      " 71  0.349848  1.935930 2.500e-04  9215.5  668.8\n",
      " 72  0.328716  1.870681 2.500e-04  9342.9  678.1\n",
      " 73  0.318340  1.943556 2.500e-04  9470.2  687.4\n",
      " 74  0.315787  1.930832 2.500e-04  9597.6  696.7\n",
      " 75  0.329697  1.907011 2.500e-04  9724.7  706.1\n",
      " 76  0.322971  1.932445 2.500e-04  9852.4  715.4\n",
      " 77  0.313876  1.948872 2.500e-04  9980.5  724.8\n",
      " 78  0.318229  1.949337 2.500e-04 10108.8  734.1\n",
      " 79  0.311733  1.917303 2.500e-04 10237.2  743.5\n",
      " 80  0.309124  1.883060 2.500e-04 10364.9  752.9\n",
      " 81  0.299804  1.924484 2.500e-04 10493.1  762.2\n",
      " 82  0.295834  1.902370 2.500e-04 10621.3  771.6\n",
      " 83  0.302329  1.883991 2.500e-04 10749.7  781.0\n",
      " 84  0.284634  1.947139 1.250e-04 10878.0  790.4\n",
      " 85  0.278279  1.891043 1.250e-04 11006.1  799.7\n",
      " 86  0.284293  1.899637 1.250e-04 11134.2  809.1\n",
      " 87  0.272770  1.930263 1.250e-04 11262.0  818.5\n",
      " 88  0.267552  1.924287 1.250e-04 11390.2  827.9\n",
      " 89  0.264892  1.900094 1.250e-04 11517.9  837.2\n",
      " 90  0.270261  1.899591 1.250e-04 11645.6  846.5\n",
      " 91  0.262490  1.894836 1.250e-04 11773.2  855.8\n",
      " 92  0.261378  1.920205 1.250e-04 11900.9  865.2\n",
      " 93  0.270170  1.931586 1.250e-04 12028.7  874.5\n",
      " 94  0.265614  1.907486 1.250e-04 12156.5  883.9\n",
      " 95  0.254764  1.904141 6.250e-05 12284.5  893.2\n",
      " 96  0.251524  1.904117 6.250e-05 12412.6  902.6\n",
      " 97  0.248953  1.903845 6.250e-05 12540.1  911.9\n",
      " 98  0.248432  1.906115 6.250e-05 12667.8  921.3\n",
      " 99  0.246350  1.908515 6.250e-05 12795.5  930.6\n",
      "100  0.251337  1.923889 6.250e-05 12922.8  939.9\n",
      "101  0.245913  1.904818 6.250e-05 13050.2  949.2\n",
      "102  0.244253  1.898226 6.250e-05 13177.4  958.5\n",
      "103  0.243426  1.899042 6.250e-05 13305.0  967.9\n",
      "104  0.242420  1.898013 6.250e-05 13432.8  977.2\n",
      "105  0.240810  1.892248 6.250e-05 13560.4  986.5\n",
      "106  0.237876  1.897922 3.125e-05 13688.2  995.9\n",
      "107  0.236788  1.901997 3.125e-05 13815.7 1005.2\n",
      "108  0.236236  1.898476 3.125e-05 13943.2 1014.5\n",
      "109  0.235840  1.899467 3.125e-05 14070.8 1023.8\n",
      "110  0.235653  1.894549 3.125e-05 14198.0 1033.2\n",
      "111  0.236491  1.893191 3.125e-05 14325.3 1042.5\n",
      "112  0.234213  1.900249 3.125e-05 14452.5 1051.8\n",
      "113  0.234015  1.895177 3.125e-05 14579.8 1061.1\n",
      "114  0.233155  1.902672 3.125e-05 14707.2 1070.4\n",
      "115  0.232563  1.898683 3.125e-05 14834.3 1079.7\n",
      "116  0.232015  1.902874 3.125e-05 14961.6 1089.0\n",
      "117  0.230379  1.898261 1.563e-05 15088.9 1098.3\n",
      "118  0.229892  1.898420 1.563e-05 15216.4 1107.7\n",
      "119  0.229980  1.897684 1.563e-05 15344.0 1117.0\n",
      "120  0.229410  1.896634 1.563e-05 15471.3 1126.3\n",
      "121  0.229279  1.895755 1.563e-05 15598.7 1135.6\n",
      "122  0.228959  1.899901 1.563e-05 15725.9 1144.9\n",
      "123  0.228521  1.898448 1.563e-05 15853.2 1154.2\n",
      "124  0.228224  1.894559 1.563e-05 15980.7 1163.5\n",
      "125  0.228016  1.896847 1.563e-05 16107.9 1172.9\n",
      "126  0.227769  1.896887 1.563e-05 16235.3 1182.2\n",
      "127  0.227543  1.895013 1.563e-05 16362.5 1191.5\n",
      "128  0.226635  1.893833 7.813e-06 16489.9 1200.8\n",
      "129  0.226325  1.892916 7.813e-06 16617.3 1210.1\n",
      "130  0.226233  1.894540 7.813e-06 16744.4 1219.4\n",
      "131  0.226013  1.895704 7.813e-06 16871.7 1228.7\n",
      "132  0.225844  1.896025 7.813e-06 16998.9 1238.0\n",
      "133  0.225782  1.894882 7.813e-06 17126.1 1247.3\n",
      "134  0.225594  1.892782 7.813e-06 17253.6 1256.7\n",
      "135  0.225429  1.895490 7.813e-06 17380.9 1266.0\n",
      "136  0.225305  1.896653 7.813e-06 17508.4 1275.3\n",
      "137  0.225123  1.897077 7.813e-06 17635.6 1284.6\n",
      "138  0.224998  1.892897 7.813e-06 17762.9 1293.9\n",
      "139  0.224543  1.895251 3.906e-06 17890.3 1303.2\n",
      "140  0.224414  1.895692 3.906e-06 18017.5 1312.5\n",
      "141  0.224374  1.894660 3.906e-06 18144.9 1321.8\n",
      "142  0.224308  1.895371 3.906e-06 18272.1 1331.2\n",
      "143  0.224181  1.894725 3.906e-06 18399.5 1340.5\n",
      "144  0.224097  1.894780 3.906e-06 18527.0 1349.8\n",
      "145  0.224030  1.895599 3.906e-06 18654.2 1359.1\n",
      "146  0.223987  1.895686 3.906e-06 18781.5 1368.4\n",
      "147  0.223895  1.894962 3.906e-06 18908.9 1377.7\n",
      "148  0.223771  1.896250 3.906e-06 19036.4 1387.0\n",
      "149  0.223726  1.895329 3.906e-06 19163.9 1396.4\n",
      "150  0.223467  1.894929 1.953e-06 19291.4 1405.7\n",
      "151  0.223413  1.895277 1.953e-06 19418.8 1415.0\n",
      "152  0.223330  1.894794 1.953e-06 19546.1 1424.3\n",
      "153  0.223345  1.895650 1.953e-06 19673.4 1433.6\n",
      "154  0.223306  1.894743 1.953e-06 19800.8 1442.9\n",
      "155  0.223225  1.894502 1.953e-06 19927.8 1452.2\n",
      "156  0.223208  1.894983 1.953e-06 20055.1 1461.5\n",
      "157  0.223178  1.896316 1.953e-06 20182.3 1470.8\n",
      "158  0.223135  1.893854 1.953e-06 20309.5 1480.1\n",
      "159  0.223048  1.896198 1.953e-06 20436.8 1489.4\n",
      "160  0.223037  1.894722 1.953e-06 20563.9 1498.7\n",
      "161  0.222884  1.894774 9.766e-07 20691.2 1508.1\n",
      "162  0.222850  1.895551 9.766e-07 20818.5 1517.4\n",
      "163  0.222891  1.894876 9.766e-07 20946.0 1526.7\n",
      "164  0.222823  1.895029 9.766e-07 21073.5 1536.0\n",
      "165  0.222784  1.895563 9.766e-07 21200.7 1545.3\n",
      "166  0.222827  1.894963 9.766e-07 21327.9 1554.6\n",
      "167  0.222816  1.894934 9.766e-07 21455.0 1563.9\n",
      "168  0.222767  1.894953 9.766e-07 21582.2 1573.2\n",
      "169  0.222757  1.894639 9.766e-07 21709.5 1582.5\n",
      "170  0.222737  1.894923 9.766e-07 21836.7 1591.8\n",
      "171  0.222674  1.894374 9.766e-07 21964.0 1601.1\n",
      "172  0.222655  1.895361 4.883e-07 22091.2 1610.4\n",
      "173  0.222637  1.894919 4.883e-07 22218.6 1619.8\n",
      "174  0.222635  1.894804 4.883e-07 22346.2 1629.1\n",
      "175  0.222551  1.894858 4.883e-07 22473.8 1638.4\n",
      "176  0.222590  1.894983 4.883e-07 22601.5 1647.8\n",
      "177  0.222561  1.895067 4.883e-07 22729.1 1657.1\n",
      "178  0.222588  1.895282 4.883e-07 22856.9 1666.4\n",
      "179  0.222569  1.894816 4.883e-07 22984.7 1675.8\n",
      "180  0.222509  1.894995 4.883e-07 23112.3 1685.1\n",
      "181  0.222526  1.894782 4.883e-07 23240.1 1694.5\n",
      "182  0.222530  1.895100 4.883e-07 23367.7 1703.8\n",
      "183  0.222488  1.894892 2.441e-07 23495.5 1713.1\n",
      "184  0.222501  1.894809 2.441e-07 23623.3 1722.5\n",
      "185  0.222496  1.894917 2.441e-07 23750.7 1731.8\n",
      "186  0.222517  1.895016 2.441e-07 23878.3 1741.1\n",
      "187  0.222459  1.894832 2.441e-07 24005.7 1750.5\n",
      "188  0.222478  1.894987 2.441e-07 24133.4 1759.8\n",
      "189  0.222435  1.894953 2.441e-07 24261.0 1769.1\n",
      "190  0.222481  1.895171 2.441e-07 24388.2 1778.4\n",
      "191  0.222474  1.895166 2.441e-07 24515.7 1787.7\n",
      "192  0.222412  1.894770 2.441e-07 24643.1 1797.1\n",
      "193  0.222442  1.894851 2.441e-07 24770.8 1806.4\n",
      "194  0.222455  1.894893 1.221e-07 24898.6 1815.8\n",
      "195  0.222407  1.894823 1.221e-07 25025.9 1825.1\n",
      "196  0.222448  1.894843 1.221e-07 25153.4 1834.4\n",
      "197  0.222448  1.894813 1.221e-07 25280.8 1843.7\n",
      "198  0.222459  1.894805 1.221e-07 25408.3 1853.1\n",
      "199  0.222350  1.894942 1.221e-07 25535.9 1862.4\n",
      "200  0.222433  1.894794 1.221e-07 25663.3 1871.7\n",
      "201  0.222433  1.894945 1.221e-07 25790.8 1881.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202  0.222432  1.894826 1.221e-07 25918.0 1890.3\n",
      "203  0.222433  1.894931 1.221e-07 26045.3 1899.6\n",
      "204  0.222413  1.894766 1.221e-07 26172.8 1909.0\n",
      "205  0.222420  1.894882 6.104e-08 26300.1 1918.3\n",
      "206  0.222402  1.894812 6.104e-08 26427.6 1927.6\n",
      "207  0.222369  1.894789 6.104e-08 26554.8 1936.9\n",
      "208  0.222400  1.894838 6.104e-08 26682.2 1946.2\n",
      "209  0.222379  1.894807 6.104e-08 26809.5 1955.5\n",
      "210  0.222386  1.894877 6.104e-08 26936.7 1964.8\n",
      "211  0.222414  1.894876 6.104e-08 27064.3 1974.2\n",
      "212  0.222421  1.894822 6.104e-08 27191.8 1983.5\n",
      "213  0.222379  1.894875 6.104e-08 27319.5 1992.8\n",
      "214  0.222414  1.894774 6.104e-08 27447.3 2002.2\n",
      "215  0.222405  1.894844 6.104e-08 27574.7 2011.5\n",
      "216  0.222379  1.894859 3.052e-08 27702.0 2020.8\n",
      "217  0.222409  1.894831 3.052e-08 27829.2 2030.1\n",
      "218  0.222374  1.894854 3.052e-08 27956.4 2039.4\n",
      "219  0.222378  1.894869 3.052e-08 28083.7 2048.7\n",
      "220  0.222356  1.894847 3.052e-08 28210.8 2058.0\n",
      "221  0.222401  1.894836 3.052e-08 28338.0 2067.3\n",
      "222  0.222383  1.894852 3.052e-08 28465.1 2076.6\n",
      "223  0.222406  1.894839 3.052e-08 28592.5 2085.9\n",
      "224  0.222370  1.894836 3.052e-08 28720.2 2095.3\n",
      "225  0.222364  1.894870 3.052e-08 28847.4 2104.6\n",
      "226  0.222350  1.894802 3.052e-08 28974.9 2113.9\n",
      "227  0.222359  1.894827 1.526e-08 29102.1 2123.2\n",
      "228  0.222407  1.894831 1.526e-08 29229.6 2132.5\n",
      "229  0.222367  1.894847 1.526e-08 29357.1 2141.9\n",
      "230  0.222387  1.894827 1.526e-08 29484.4 2151.2\n",
      "231  0.222424  1.894846 1.526e-08 29611.9 2160.5\n",
      "232  0.222380  1.894822 1.526e-08 29739.1 2169.8\n",
      "233  0.222409  1.894834 1.526e-08 29866.6 2179.1\n",
      "234  0.222387  1.894834 1.526e-08 29994.2 2188.4\n",
      "235  0.222416  1.894850 1.526e-08 30121.5 2197.8\n",
      "236  0.222402  1.894827 1.526e-08 30249.1 2207.1\n",
      "237  0.222354  1.894831 1.526e-08 30376.4 2216.4\n",
      "238  0.222368  1.894840 1.526e-08 30503.9 2225.7\n",
      "239  0.222388  1.894831 1.526e-08 30631.4 2235.0\n",
      "240  0.222395  1.894849 1.526e-08 30758.6 2244.4\n",
      "241  0.222359  1.894819 1.526e-08 30886.1 2253.7\n",
      "242  0.222359  1.894827 1.526e-08 31013.3 2263.0\n",
      "243  0.222370  1.894823 1.526e-08 31140.6 2272.3\n",
      "244  0.222403  1.894831 1.526e-08 31268.0 2281.6\n",
      "245  0.222370  1.894827 1.526e-08 31395.1 2290.9\n",
      "246  0.222360  1.894829 1.526e-08 31522.3 2300.2\n",
      "247  0.222381  1.894831 1.526e-08 31649.4 2309.5\n",
      "248  0.222334  1.894836 1.526e-08 31776.6 2318.8\n",
      "249  0.222322  1.894826 1.526e-08 31903.9 2328.1\n",
      "250  0.222370  1.894828 1.526e-08 32031.0 2337.4\n",
      "251  0.222371  1.894825 1.526e-08 32158.2 2346.7\n",
      "252  0.222356  1.894826 1.526e-08 32285.2 2356.0\n",
      "253  0.222360  1.894811 1.526e-08 32412.4 2365.3\n",
      "254  0.222403  1.894815 1.526e-08 32539.9 2374.6\n",
      "255  0.222389  1.894836 1.526e-08 32667.1 2383.9\n",
      "256  0.222395  1.894832 1.526e-08 32794.3 2393.2\n",
      "257  0.222355  1.894820 1.526e-08 32921.4 2402.5\n",
      "258  0.222408  1.894825 1.526e-08 33048.6 2411.8\n",
      "259  0.222354  1.894829 1.526e-08 33175.8 2421.1\n",
      "260  0.222354  1.894824 1.526e-08 33302.8 2430.4\n",
      "261  0.222372  1.894847 1.526e-08 33430.0 2439.7\n",
      "262  0.222326  1.894821 1.526e-08 33557.0 2449.0\n",
      "263  0.222358  1.894826 1.526e-08 33684.2 2458.3\n",
      "264  0.222385  1.894824 1.526e-08 33811.5 2467.6\n",
      "265  0.222380  1.894823 1.526e-08 33938.6 2476.9\n",
      "266  0.222331  1.894832 1.526e-08 34065.7 2486.2\n",
      "267  0.222345  1.894836 1.526e-08 34192.7 2495.5\n",
      "268  0.222405  1.894830 1.526e-08 34319.8 2504.8\n",
      "269  0.222330  1.894830 1.526e-08 34447.1 2514.1\n",
      "270  0.222382  1.894818 1.526e-08 34574.0 2523.4\n",
      "271  0.222371  1.894833 1.526e-08 34701.2 2532.7\n",
      "272  0.222344  1.894820 1.526e-08 34828.1 2542.0\n",
      "273  0.222372  1.894830 1.526e-08 34955.2 2551.3\n",
      "274  0.222360  1.894825 1.526e-08 35082.4 2560.6\n",
      "275  0.222312  1.894829 1.526e-08 35209.3 2569.9\n",
      "276  0.222360  1.894828 1.526e-08 35336.7 2579.2\n",
      "277  0.222401  1.894819 1.526e-08 35463.8 2588.5\n",
      "278  0.222343  1.894811 1.526e-08 35590.9 2597.7\n",
      "279  0.222373  1.894828 1.526e-08 35718.1 2607.0\n",
      "280  0.222318  1.894842 1.526e-08 35845.0 2616.3\n",
      "281  0.222328  1.894833 1.526e-08 35972.1 2625.6\n",
      "282  0.222337  1.894823 1.526e-08 36099.1 2634.9\n",
      "283  0.222331  1.894827 1.526e-08 36226.1 2644.2\n",
      "284  0.222364  1.894832 1.526e-08 36353.3 2653.5\n",
      "285  0.222332  1.894830 1.526e-08 36480.2 2662.8\n",
      "286  0.222325  1.894827 1.526e-08 36609.6 2672.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30709/2043671290.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mloss_train\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mn_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nfold = 5\n",
    "kfold = KFold(n_splits=nfold, shuffle=True, random_state=228)\n",
    "epochs = 2 if debug else 300\n",
    "lr = 5e-4\n",
    "batch_size = 256\n",
    "max_grad_norm = 1000\n",
    "log = {}\n",
    "\n",
    "for ifold, (idx_train, idx_val) in enumerate(kfold.split(train)):\n",
    "    print('Fold %d' % ifold)\n",
    "    tb = time.time()\n",
    "    model = Model(input_size)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.5, patience=10)\n",
    "\n",
    "    train_fold, val_fold = train.iloc[idx_train,:],train.iloc[idx_val,:]\n",
    "    features = create_features(train_fold)\n",
    "    rs = sklearn.preprocessing.RobustScaler()\n",
    "    features = rs.fit_transform(features)  # => np.ndarray\n",
    "\n",
    "    X_train = features.reshape(-1, 80, features.shape[-1])\n",
    "    y_train = train_fold.pressure.values.reshape(-1, 80)\n",
    "    w_train = 1 - train_fold.u_out.values.reshape(-1, 80)  # weights for the score, but not used in this notebook\n",
    "    \n",
    "    X_val = create_features(val_fold)\n",
    "    X_val = rs.transform(X_val)\n",
    "    X_val = X_val.reshape(-1, 80, features.shape[-1])\n",
    "    y_val = val_fold.pressure.values.reshape(-1, 80)\n",
    "    w_val = 1 - val_fold.u_out.values.reshape(-1, 80)  # weights for the score, but not used in this notebook\n",
    "    \n",
    "    #     X_train = X_all[idx_train]\n",
    "    #     y_train = y_all[idx_train]\n",
    "    #     w_train = w_all[idx_train]\n",
    "\n",
    "    #     X_val = X_all[idx_val]\n",
    "    #     y_val = y_all[idx_val]\n",
    "    #     w_val = w_all[idx_val]\n",
    "\n",
    "    dataset_train = Dataset(X_train, y_train, w_train)\n",
    "    dataset_val = Dataset(X_val, y_val, w_val)\n",
    "    loader_train = torch.utils.data.DataLoader(dataset_train, shuffle=True,\n",
    "                         batch_size=batch_size, drop_last=True)\n",
    "    loader_val = torch.utils.data.DataLoader(dataset_val, shuffle=False,\n",
    "                         batch_size=batch_size, drop_last=False)\n",
    "\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "    lrs = []\n",
    "    time_val = 0\n",
    "    best_score = np.inf\n",
    "   \n",
    "    print('epoch loss_train loss_val lr time')\n",
    "    for iepoch in range(epochs):\n",
    "        loss_train = 0\n",
    "        n_sum = 0\n",
    "        \n",
    "        for ibatch, (x, y, w) in enumerate(loader_train):\n",
    "            n = y.size(0)\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = model(x).squeeze()\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss_train += n*loss.item()\n",
    "            n_sum += n\n",
    "\n",
    "            loss.backward()\n",
    "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        val = evaluate(model, loader_val)\n",
    "        loss_val = val['loss']\n",
    "        time_val += val['time']\n",
    "\n",
    "        losses_train.append(loss_train / n_sum)\n",
    "        losses_val.append(val['loss'])\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        print('%3d %9.6f %9.6f %7.3e %7.1f %6.1f' %\n",
    "              (iepoch + 1,\n",
    "               losses_train[-1], losses_val[-1], \n",
    "               lrs[-1], time.time() - tb, time_val))\n",
    "\n",
    "        scheduler.step(losses_val[-1])\n",
    "\n",
    "\n",
    "    ofilename = 'model%d.pth' % ifold\n",
    "    torch.save(model.state_dict(), ofilename)\n",
    "    print(ofilename, 'written')\n",
    "\n",
    "    log['fold%d' % ifold] = {\n",
    "        'loss_train': np.array(losses_train),\n",
    "        'loss_val': np.array(losses_val),\n",
    "        'learning_rate': np.array(lrs),\n",
    "        'y_pred': val['y_pred'],\n",
    "        'idx': idx_val\n",
    "    }\n",
    "    \n",
    "    if ifold >= 1: # due to time limit\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23008ed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:33:28.460504Z",
     "start_time": "2021-10-13T14:33:28.460495Z"
    },
    "execution": {
     "iopub.execute_input": "2021-10-12T07:01:44.344732Z",
     "iopub.status.busy": "2021-10-12T07:01:44.344049Z",
     "iopub.status.idle": "2021-10-12T07:01:44.346827Z",
     "shell.execute_reply": "2021-10-12T07:01:44.347241Z",
     "shell.execute_reply.started": "2021-10-12T01:58:05.742829Z"
    },
    "papermill": {
     "duration": 0.181792,
     "end_time": "2021-10-12T07:01:44.347372",
     "exception": false,
     "start_time": "2021-10-12T07:01:44.165580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Fold loss_train loss_val best loss_val')\n",
    "for ifold in range(2):\n",
    "    d = log['fold%d' % ifold]\n",
    "    print('%4d %9.6f %9.6f %9.6f' % (ifold, d['loss_train'][-1], d['loss_val'][-1], np.min(d['loss_val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfda0d84",
   "metadata": {
    "papermill": {
     "duration": 0.168013,
     "end_time": "2021-10-12T07:01:44.676622",
     "exception": false,
     "start_time": "2021-10-12T07:01:44.508609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I trained 5 folds locally,\n",
    "\n",
    "```\n",
    "epoch loss_train loss_val\n",
    "0.119303  0.184425 \n",
    "0.105525  0.184154\n",
    "0.109591  0.179805\n",
    "0.127961  0.191654\n",
    "0.141102  0.202042\n",
    "```\n",
    "\n",
    "The original TensorFlow scores at the end are,\n",
    "\n",
    "```\n",
    "loss val_loss (epoch 300)\n",
    "0.1351 0.1902\n",
    "0.1365 0.1897\n",
    "0.1292 0.1972\n",
    "0.1221 0.1970\n",
    "0.1276 0.1976\n",
    "```\n",
    "\n",
    "I am satisfied with the similarity.\n",
    "\n",
    "Note that the loss here is the overall MAE including the expiratory phase, which is not the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1a288a",
   "metadata": {
    "papermill": {
     "duration": 0.161272,
     "end_time": "2021-10-12T07:01:44.999615",
     "exception": false,
     "start_time": "2021-10-12T07:01:44.838343",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Predict and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33e1fd47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T07:01:45.332980Z",
     "iopub.status.busy": "2021-10-12T07:01:45.331997Z",
     "iopub.status.idle": "2021-10-12T07:05:36.540026Z",
     "shell.execute_reply": "2021-10-12T07:05:36.540594Z",
     "shell.execute_reply.started": "2021-10-12T01:58:05.751240Z"
    },
    "papermill": {
     "duration": 231.378616,
     "end_time": "2021-10-12T07:05:36.540790",
     "exception": false,
     "start_time": "2021-10-12T07:01:45.162174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv written\n"
     ]
    }
   ],
   "source": [
    "features = create_features(test)\n",
    "features = rs.transform(features)\n",
    "\n",
    "X_test = features.reshape(-1, 80, features.shape[-1])\n",
    "y_test = np.zeros(len(features)).reshape(-1, 80)\n",
    "w_test = 1 - test.u_out.values.reshape(-1, 80)\n",
    "\n",
    "dataset_test = Dataset(X_test, y_test, w_test)\n",
    "loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size)\n",
    "\n",
    "y_pred_folds = np.zeros((len(test), 5), dtype=np.float32)\n",
    "for ifold in range(5):\n",
    "    model = Model(input_size)\n",
    "    model.to(device)\n",
    "    filename = '/kaggle/input/pytorchlstmwithtensorflowlikeinitialization/' \\\n",
    "               'model%d.pth' % ifold\n",
    "    model.load_state_dict(torch.load(filename, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    y_preds = []\n",
    "    for x, y, _ in loader_test:\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x).squeeze()\n",
    "\n",
    "        y_preds.append(y_pred.cpu().numpy())\n",
    "    \n",
    "    y_preds = np.concatenate(y_preds, axis=0)\n",
    "    y_pred_folds[:, ifold] = y_preds.flatten()\n",
    "\n",
    "submit.pressure = np.mean(y_pred_folds, axis=1)\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "print('submission.csv written')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2768dee",
   "metadata": {
    "papermill": {
     "duration": 0.164778,
     "end_time": "2021-10-12T07:05:36.875775",
     "exception": false,
     "start_time": "2021-10-12T07:05:36.710997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Minor differences from the original, with no reason.\n",
    "\n",
    "* Learning rate scheduler is ReduceLROnPlateau, which is used in [other notebooks](https://www.kaggle.com/tenffe/finetune-of-tensorflow-bidirectional-lstm);\n",
    "* I have not implemented early stopping;\n",
    "* random seeds other than kfold are not fixed in the original. Mine is not strictly deterministic either.\n",
    "\n",
    "There are more features, loss function, or better aggrigation of nfold predictions, and so on, in public notebooks, but my goal here is to reproduce score as good as TensorFlow using same model and features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18318.533399,
   "end_time": "2021-10-12T07:05:38.051156",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-12T02:00:19.517757",
   "version": "2.3.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
